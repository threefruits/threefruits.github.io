<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research - Anxing Xiao</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="styles/globals.css">
    <style>
        svg a {
            text-decoration: none;
        }
        svg a .block-rect {
            transition: fill 0.2s, opacity 0.2s;
        }
        svg a:hover .block-rect {
            fill: rgba(133, 160, 173, 0.85);
        }
        html {
            scroll-behavior: smooth;
        }
    </style>
</head>
<body>
    <div class="background-image"></div>

    <header class="bg-gradient-to-r from-teal-700 to-blue-900 shadow-md sticky top-0 z-50">
        <div class="max-w-7xl mx-auto px-8 py-4">
            <div class="flex items-center justify-between">
                <h1 class="text-white">Anxing Xiao</h1>
                <nav class="flex gap-4">
                    <a href="index.html" class="inline-block px-3 py-1 rounded-md text-white hover:text-white hover:bg-blue-200/10 transition-colors">About Me</a>
                    <a href="research.html" class="inline-block px-3 py-1 rounded-md text-white hover:text-white hover:bg-blue-200/10 transition-colors">Research</a>
                    <a href="publications.html" class="inline-block px-3 py-1 rounded-md text-white hover:text-white hover:bg-blue-200/10 transition-colors">Publications</a>
                    <a href="misc.html" class="inline-block px-3 py-1 rounded-md text-white hover:text-white hover:bg-blue-200/10 transition-colors">Misc</a>
                    <a href="join.html" class="inline-block px-3 py-1 rounded-md text-white hover:text-white hover:bg-blue-200/10 transition-colors">Join Us</a>
                </nav>
            </div>
        </div>
    </header>

    <main class="py-8">
        <section class="max-w-7xl mx-auto px-8 py-2">
            <div class="bg-white/60 backdrop-blur-sm p-8 rounded-lg shadow-lg space-y-0">
                <div>
                    <h2 class="text-gray-800 mb-1">RESEARCH</h2>
                    <div class="border-t-2 border-gray-400 w-24 mb-4"></div>
                    <div class="space-y-4 text-gray-700 leading-relaxed">
                        <p>My research interests lie in developing both general-purpose algorithms and specialized autonomous systems for service robots, including planning, human-robot interaction, and service robot design. I aim to bridge the gap between robotics research and its real-world applications in socially aware scenarios.</p>
                        <p class="mt-0">My long-term research goal is to develop <strong>deployable</strong> service robotic systems capable of operating in <strong>open-world</strong> environments. I define the open world as one characterized by (1) natural and unconstrained human behavior, (2) incomplete environmental knowledge at both the semantic and geometric levels, and (3) unknown environmental structures populated with novel objects.</p>
                        <p class="mt-0">To tackle the challenges of open-world operation, I adopt the system design grounded in <strong>compositionality</strong> and <strong>modularity</strong>. I explore structured representations that link semantic concepts with their grounding in the physical world. At the low level, I develop robust navigation and manipulation systems that passively tolerate unforeseen objects and environments. At the high level, I design systems and algorithms that actively expand the robot's representations and models, and incorporate human presence at both levels.</p>
                    </div>
                </div>

                <div>
                    <div class="overflow-x-auto">
                        <svg width="1400" height="330" viewBox="0 0 1400 330" class="max-w-full">
                            <line x1="530" y1="85" x2="200" y2="180" stroke="black" stroke-width="2" opacity="0.6"/>
                            <text x="300" y="85" text-anchor="middle" font-size="13" fill="black" >Socially-Aware Navigation/Exploration</text>
                            <text x="300" y="105" text-anchor="middle" font-size="13" fill="black" >Multimodal Domain Induction for TAMP</text>

                            <line x1="870" y1="86" x2="1200" y2="180" stroke="black" stroke-width="2" opacity="0.6"/>
                            <text x="1095" y="85" text-anchor="middle" font-size="13" fill="black" >Mobile Manipulation in the Changing Environment</text>
                            <text x="1095" y="105" text-anchor="middle" font-size="13" fill="black" >Planning with Partial World Knowledge</text>

                            <line x1="370" y1="265" x2="1030" y2="265" stroke="black" stroke-width="2" opacity="0.6"/>
                            <text x="700" y="285" text-anchor="middle" font-size="13" fill="black" >Multimodal Instructions Grounding</text>
                            <text x="700" y="305" text-anchor="middle" font-size="13" fill="black" >Planning and Control for Physical HRI</text>
                            <text x="700" y="325" text-anchor="middle" font-size="13" fill="black" >Learning from Human Videos</text>

                            <a href="#open-world-interaction" style="cursor: pointer;">
                                <g class="research-block" data-area="planning">
                                    <rect x="30" y="180" width="340" height="170" rx="0" fill="rgba(133, 160, 173, 0.649)" stroke="none" class="block-rect"/>
                                    <text x="200" y="225" text-anchor="middle" font-size="18" font-weight="bold" fill="#191818">Open-world Interaction</text>
                                    <text x="200" y="265" text-anchor="middle" font-size="13" fill="#191818">Enabling seamless human-robot interaction</text>
                                    <text x="200" y="295" text-anchor="middle" font-size="13" fill="#191818"> in daily assistive tasks by developing</text>
                                    <text x="200" y="325" text-anchor="middle" font-size="13" fill="#191818">algorithms that reduce human effort.</text>
                                </g>
                            </a>

                            <a href="#open-world-reasoning" style="cursor: pointer;">
                                <g class="research-block" data-area="social">
                                    <rect x="530" y="0" width="340" height="170" rx="0" fill="rgba(133, 160, 173, 0.649)" stroke="none" class="block-rect"/>
                                    <text x="700" y="45" text-anchor="middle" font-size="18" font-weight="bold" fill="#191818">Open-world Reasoning</text>
                                    <text x="700" y="85" text-anchor="middle" font-size="13" fill="#191818">Enabling robots to reason and</text>
                                    <text x="700" y="115" text-anchor="middle" font-size="13" fill="#191818">generate plans with limited knowledge</text>
                                    <text x="700" y="145" text-anchor="middle" font-size="13" fill="#191818">about the unknown environment.</text>
                                </g>
                            </a>

                            <a href="#open-world-capability" style="cursor: pointer;">
                                <g class="research-block" data-area="navigation">
                                    <rect x="1030" y="180" width="340" height="170" rx="0" fill="rgba(133, 160, 173, 0.649)" stroke="none" class="block-rect"/>
                                    <text x="1200" y="225" text-anchor="middle" font-size="18" font-weight="bold" fill="#191818">Open-world Capability</text>
                                    <text x="1200" y="265" text-anchor="middle" font-size="13" fill="#191818">Empowering robots the ability to perform</text>
                                    <text x="1200" y="295" text-anchor="middle" font-size="13" fill="#191818">generalizable navigation/manipulation</text>
                                    <text x="1200" y="325" text-anchor="middle" font-size="13" fill="#191818">skills in unknown environments.</text>
                                </g>
                            </a>

                            <circle cx="530" cy="120" r="4" fill="transparent"/>
                            <circle cx="870" cy="120" r="4" fill="transparent"/>
                            <circle cx="200" cy="180" r="4" fill="transparent"/>
                            <circle cx="1200" cy="180" r="4" fill="transparent"/>
                            <circle cx="370" cy="250" r="4" fill="transparent"/>
                            <circle cx="1030" cy="250" r="4" fill="transparent"/>
                        </svg>
                    </div>
                </div>
            </div>
        </section>

        <section id="open-world-interaction" class="max-w-7xl mx-auto px-8 py-7">
            <div class="bg-white/60 backdrop-blur-sm p-8 rounded-lg shadow-lg space-y-4">
                <h2 class="text-gray-800">Open-world Interaction</h2>
                <div class="border-t-2 border-gray-400 w-24 mb-4"></div>
                <div class="space-y-4">
                    <div class="bg-white/60 p-6 rounded-lg border border-gray-200">
                        <div class="flex flex-row gap-6">
                            <div class="w-64 flex-shrink-0">
                                <img src="assets/img/demo/gson.gif" class="w-full aspect-[16/9] object-cover rounded" alt="GSON">
                            </div>
                            <div class="flex-1 space-y-1">
                                <h3 class="text-gray-800">GSON: A Group-based Social Navigation Framework with Large Multimodal Model</h3>
                                <p class="text-gray-700 italic">IEEE Robotics and Automation Letters (RA-L) 2025.</p>
                                <p class="text-gray-600">Shangyi Luo, Peng Sun, Ji Zhu, Yuhong Deng, Cunjun Yu, <strong>Anxing Xiao</strong> (Mentor), Xueqian Wang</p>
                                <div class="pt-1">
                                    <p class="text-gray-700"><strong>TL;DR:</strong> Group-aware social navigation using large multimodal models for socially-aware robot motion.</p>
                                </div>
                                <div class="flex flex-wrap gap-2 pt-1">
                                    <a href="https://arxiv.org/pdf/2409.18084" class="button" target="_blank">Paper</a>
                                    <a href="https://www.youtube.com/watch?v=S_o-hvIBg5M" class="button" target="_blank">Video</a>
                                    <a href="https://github.com/lsylsy0516/GSON" class="button" target="_blank">Code</a>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="bg-white/60 p-6 rounded-lg border border-gray-200">
                        <div class="flex flex-row gap-6">
                            <div class="w-64 flex-shrink-0">
                                <img src="assets/img/demo/robibutler.gif" class="w-full aspect-[16/9] object-cover rounded" alt="Robi Butler">
                            </div>
                            <div class="flex-1 space-y-1">
                                <h3 class="text-gray-800">Robi Butler: Multimodal Remote Interactions with a Household Robot Assistant</h3>
                                <p class="text-gray-700 italic">IEEE International Conference on Robotics and Automation (ICRA) 2025.</p>
                                <p class="text-gray-600"><strong>Anxing Xiao</strong>, Nuwan Janaka, Tianrun Hu, Anshul Gupta, Kaixin Li, Cunjun Yu, David Hsu</p>
                                <div class="pt-1">
                                    <p class="text-gray-700"><strong>TL;DR:</strong> Remote multimodal (language + gesture) interactions with household robots.</p>
                                </div>
                                <div class="flex flex-wrap gap-2 pt-1">
                                    <a href="https://arxiv.org/abs/2409.20548" class="button" target="_blank">Paper</a>
                                    <a href="https://youtu.be/kz6P3ui-s3I?si=oXWV54YuRMzCRsQL" class="button" target="_blank">Video</a>
                                    <a href="https://robibutler.github.io/" class="button" target="_blank">Website</a>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="bg-white/60 p-6 rounded-lg border border-gray-200">
                        <div class="flex flex-row gap-6">
                            <div class="w-64 flex-shrink-0">
                                <img src="assets/img/demo/guidedog2.gif" class="w-full aspect-[16/9] object-cover rounded" alt="Quadruped Guidance Robot">
                            </div>
                            <div class="flex-1 space-y-1">
                                <h3 class="text-gray-800">Quadruped Guidance Robot for the Visually Impaired: A Comfort-Based Approach</h3>
                                <p class="text-gray-700 italic">IEEE International Conference on Robotics and Automation (ICRA) 2023.</p>
                                <p class="text-gray-600">Yanbo Chen, Zhengzhe Xu, Zhuozhu Jian, Gengpan Tang, Yunong Yangli, <strong>Anxing Xiao</strong> (Mentor), Xueqian Wang, Bin Liang</p>
                                <div class="pt-1">
                                    <p class="text-gray-700"><strong>TL;DR:</strong> Comfort-based guidance that safely leads visually impaired users through complex environments.</p>
                                </div>
                                <div class="flex flex-wrap gap-2 pt-1">
                                    <a href="https://arxiv.org/abs/2203.03927" class="button" target="_blank">Paper</a>
                                    <a href="https://www.youtube.com/watch?v=Xroov-UASC0" class="button" target="_blank">Video</a>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="bg-white/60 p-6 rounded-lg border border-gray-200">
                        <div class="flex flex-row gap-6">
                            <div class="w-64 flex-shrink-0">
                                <img src="assets/img/demo/guidedog1.gif" 
                                     alt="Robotic Guide Dog" 
                                     class="w-full aspect-[16/9] object-cover rounded">
                            </div>
                            
                            <div class="flex-1 space-y-1">
                                <h3 class="text-gray-800">Robotic Guide Dog: Leading a Human with Leash-Guided Hybrid Physical Interactions</h3>
                                <p class="text-gray-700 italic">IEEE International Conference on Robotics and Automation (ICRA) 2021.</p>
                                <p class="text-gray-600"><strong>Anxing Xiao</strong>, Wenzhe Tong, Lizhi Yang, Jun Zeng, Zhongyu Li, Koushil Sreenath</p>
                                <p class="text-gray-600"><span class="text-orange-600"> ICRA Best Paper Award Finalist for Service Robotics.</span></p>
                                <div class="pt-0">
                                    <p class="text-gray-700">
                                        <strong>TL;DR:</strong> The world's first robotic guide dog that can lead visually impaired to safely travel in a confined space.
                                    </p>
                                </div>
                                
                                <div class="flex flex-wrap gap-2 pt-1">
                                    <a href="https://arxiv.org/abs/2103.14300" class="button" target="_blank">Paper</a>
                                    <a href="https://www.youtube.com/watch?v=FySXRzmji8Y&t=11s" class="button" target="_blank">Video</a>
                                </div>
                            </div>
                        </div>
                    </div>
                    
                </div>
            </div>
        </section>
        <section id="open-world-reasoning" class="max-w-7xl mx-auto px-8 py-7">
            <div class="bg-white/60 backdrop-blur-sm p-8 rounded-lg shadow-lg space-y-4">
                <h2 class="text-gray-800">Open-world Reasoning</h2>
                <div class="border-t-2 border-gray-400 w-24 mb-4"></div>
                <div class="space-y-4">
                    <div class="bg-white/60 p-6 rounded-lg border border-gray-200">
                        <div class="flex flex-row gap-6">
                            <div class="w-64 flex-shrink-0">
                                <img src="assets/img/demo/apbot.gif" class="w-full aspect-[16/9] object-cover rounded" alt="ApBot">
                            </div>
                            <div class="flex-1 space-y-1">
                                <h3 class="text-gray-800">Robot Operation of Home Appliances by Reading User Manuals</h3>
                                <p class="text-gray-700 italic">Conference on Robot Learning (CoRL) 2025.</p>
                                <p class="text-gray-600">Jian Zhang, Hanbo Zhang, <strong>Anxing Xiao</strong>, David Hsu</p>
                                <div class="pt-1">
                                    <p class="text-gray-700"><strong>TL;DR:</strong> Read and follow user manuals to operate home appliances autonomously.</p>
                                </div>
                                <div class="flex flex-wrap gap-2 pt-1">
                                    <a href="https://arxiv.org/abs/2505.20424" class="button" target="_blank">Paper</a>
                                    <a href="https://www.youtube.com/watch?v=lk7SyVyE380" class="button" target="_blank">Video</a>
                                    <a href="https://github.com/zhangj1an/ApBot" class="button" target="_blank">Code</a>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="bg-white/60 p-6 rounded-lg border border-gray-200">
                        <div class="flex flex-row gap-6">
                            <div class="w-64 flex-shrink-0">
                                <img src="assets/img/demo/octopi_demo.gif" class="w-full aspect-[16/9] object-cover rounded" alt="Octopi">
                            </div>
                            <div class="flex-1 space-y-1">
                                <h3 class="text-gray-800">Octopi: Object Property Reasoning with Large Tactile-Language Models</h3>
                                <p class="text-gray-700 italic">Robotics: Science and Systems (RSS) 2024.</p>
                                <p class="text-gray-600">Samson Yu, Kelvin Lin, <strong>Anxing Xiao</strong>, Jiafei Duan, and Harold Soh</p>
                                <div class="pt-1">
                                    <p class="text-gray-700"><strong>TL;DR:</strong> Combine tactile sensing with language to infer physical properties via interaction.</p>
                                </div>
                                <div class="flex flex-wrap gap-2 pt-1">
                                    <a href="https://arxiv.org/pdf/2405.02794" class="button" target="_blank">Paper</a>
                                    <a href="https://octopi-tactile-lvlm.github.io/" class="button" target="_blank">Website</a>
                                    <a href="https://github.com/clear-nus/octopi" class="button" target="_blank">Code</a>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="bg-white/60 p-6 rounded-lg border border-gray-200">
                        <div class="flex flex-row gap-6">
                            <div class="w-64 flex-shrink-0">
                                <img src="assets/img/demo/llmstate.gif" class="w-full aspect-[16/9] object-cover rounded" alt="LLM-State">
                            </div>
                            <div class="flex-1 space-y-1">
                                <h3 class="text-gray-800">LLM-State: Expandable State Representation for Long-horizon Task Planning in the Open World</h3>
                                <p class="text-gray-700 italic">Preprint 2024.</p>
                                <p class="text-gray-600">Siwei Chen, <strong>Anxing Xiao</strong>, David Hsu</p>
                                <div class="pt-1">
                                    <p class="text-gray-700"><strong>TL;DR:</strong> Maintain and expand object attributes for long-horizon planning in open worlds.</p>
                                </div>
                                <div class="flex flex-wrap gap-2 pt-1">
                                    <a href="https://arxiv.org/pdf/2311.17406" class="button" target="_blank">Paper</a>
                                    <a href="https://www.youtube.com/watch?v=QkN-8pxV3Mo" class="button" target="_blank">Video</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        <section id="open-world-capability" class="max-w-7xl mx-auto px-8 py-7">
            <div class="bg-white/60 backdrop-blur-sm p-8 rounded-lg shadow-lg space-y-4">
                <h2 class="text-gray-800">Open-world Capability</h2>
                <div class="border-t-2 border-gray-400 w-24 mb-4"></div>
                <div class="space-y-4">
                    <div class="bg-white/60 p-6 rounded-lg border border-gray-200">
                        <div class="flex flex-row gap-6">
                            <div class="w-64 flex-shrink-0">
                                <img src="assets/img/demo/mimicfunc.gif" class="w-full aspect-[16/9] object-cover rounded" alt="MimicFunc">
                            </div>
                            <div class="flex-1 space-y-1">
                                <h3 class="text-gray-800">MimicFunc: Imitating Tool Manipulation from a Single Human Video via Functional Correspondence</h3>
                                <p class="text-gray-700 italic">Conference on Robot Learning (CoRL) 2025.</p>
                                <p class="text-gray-600">Chao Tang, <strong>Anxing Xiao</strong>, Yuhong Deng, Tianrun Hu, Wenlong Dong, Hanbo Zhang, David Hsu, Hong Zhang</p>
                                <div class="pt-1">
                                    <p class="text-gray-700"><strong>TL;DR:</strong> Learn tool-use from a single human video via functional correspondence.</p>
                                </div>
                                <div class="flex flex-wrap gap-2 pt-1">
                                    <a href="https://www.arxiv.org/abs/2508.13534" class="button" target="_blank">Paper</a>
                                    <a href="https://sites.google.com/view/mimicfunc" class="button" target="_blank">Website</a>
                                    <a href="https://github.com/mkt1412/FUNCTO_public" class="button" target="_blank">Code</a>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="bg-white/60 p-6 rounded-lg border border-gray-200">
                        <div class="flex flex-row gap-6">
                            <div class="w-64 flex-shrink-0">
                                <img src="assets/img/demo/chd.gif" class="w-full aspect-[16/9] object-cover rounded" alt="CHD">
                            </div>
                            <div class="flex-1 space-y-1">
                                <h3 class="text-gray-800">CHD: Coupled Hierarchical Diffusion for Long-Horizon Tasks</h3>
                                <p class="text-gray-700 italic">Conference on Robot Learning (CoRL) 2025.</p>
                                <p class="text-gray-600">Ce Hao, <strong>Anxing Xiao</strong>, Zhiwei Xue, Harold Soh</p>
                                <div class="pt-1">
                                    <p class="text-gray-700"><strong>TL;DR:</strong> Hierarchical diffusion for planning and executing long-horizon robotic tasks.</p>
                                </div>
                                <div class="flex flex-wrap gap-2 pt-1">
                                    <a href="https://arxiv.org/pdf/2505.07261" class="button" target="_blank">Paper</a>
                                    <a href="https://www.youtube.com/watch?v=08SWuL0ZwDA" class="button" target="_blank">Video</a>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="bg-white/60 p-6 rounded-lg border border-gray-200">
                        <div class="flex flex-row gap-6">
                            <div class="w-64 flex-shrink-0">
                                <img src="assets/img/demo/trolley_collect.gif" class="w-full aspect-[16/9] object-cover rounded" alt="Trolley Collection">
                            </div>
                            <div class="flex-1 space-y-1">
                                <h3 class="text-gray-800">Robotic Autonomous Trolley Collection with Progressive Perception and Nonlinear Model Predictive Control</h3>
                                <p class="text-gray-700 italic">IEEE International Conference on Robotics and Automation (ICRA) 2022.</p>
                                <p class="text-gray-600"><strong>Anxing Xiao</strong>*, Hao Luan*, Ziqi Zhao*, Yue Hong, Jieting Zhao, Jiankun Wang, Max Q-H Meng</p>
                                <div class="pt-1">
                                    <p class="text-gray-700"><strong>TL;DR:</strong> Compact robot collects unmarked trolleys in dynamic, complex environments.</p>
                                </div>
                                <div class="flex flex-wrap gap-2 pt-1">
                                    <a href="https://arxiv.org/abs/2110.06648" class="button" target="_blank">Paper</a>
                                    <a href="https://www.youtube.com/watch?v=6SwjgGvRtno" class="button" target="_blank">Video</a>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="bg-white/60 p-6 rounded-lg border border-gray-200">
                        <div class="flex flex-row gap-6">
                            <div class="w-64 flex-shrink-0">
                                <img src="assets/img/demo/trolley_trans.gif" class="w-full aspect-[16/9] object-cover rounded" alt="Collaborative Trolley Transportation">
                            </div>
                            <div class="flex-1 space-y-1">
                                <h3 class="text-gray-800">Collaborative Trolley Transportation System with Autonomous Nonholonomic Robots</h3>
                                <p class="text-gray-700 italic">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2023.</p>
                                <p class="text-gray-600">Bingyi Xia, Hao Luan, Ziqi Zhao, Xuheng Gao, Peijia Xie, <strong>Anxing Xiao</strong> (Mentor), Jiankun Wang, Max Q-H Meng</p>
                                <div class="pt-1">
                                    <p class="text-gray-700"><strong>TL;DR:</strong> Multi-robot system for collaborative trolley transportation in crowded spaces.</p>
                                </div>
                                <div class="flex flex-wrap gap-2 pt-1">
                                    <a href="https://arxiv.org/abs/2303.06624" class="button" target="_blank">Paper</a>
                                    <a href="https://www.youtube.com/watch?v=efnPERm0Rco&feature=youtu.be" class="button" target="_blank">Video</a>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="bg-white/60 p-6 rounded-lg border border-gray-200">
                        <div class="flex flex-row gap-6">
                            <div class="w-64 flex-shrink-0">
                                <img src="assets/img/demo/navigation.gif" class="w-full aspect-[16/9] object-cover rounded" alt="PUTN">
                            </div>
                            <div class="flex-1 space-y-1">
                                <h3 class="text-gray-800">PUTN: A Plane-fitting based Uneven Terrain Navigation Framework</h3>
                                <p class="text-gray-700 italic">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2022.</p>
                                <p class="text-gray-600">Zhuozhu Jian, Zihong Lu, Xiao Zhou, Bin Lan, <strong>Anxing Xiao</strong> (Mentor), Xueqian Wang, Bin Liang</p>
                                <div class="pt-1">
                                    <p class="text-gray-700"><strong>TL;DR:</strong> Plane-fitting based navigation for effective traversal on uneven terrain.</p>
                                </div>
                                <div class="flex flex-wrap gap-2 pt-1">
                                    <a href="https://arxiv.org/abs/2203.04541" class="button" target="_blank">Paper</a>
                                    <a href="https://www.youtube.com/watch?v=3ZK-Ut29hLI" class="button" target="_blank">Video</a>
                                    <a href="https://github.com/jianzhuozhuTHU/putn" class="button" target="_blank">Code</a>
                                </div>
                            </div>
                        </div>
                    </div>

                    <div class="bg-white/60 p-6 rounded-lg border border-gray-200">
                        <div class="flex flex-row gap-6">
                            <div class="w-64 flex-shrink-0">
                                <img src="assets/img/demo/jump.gif" class="w-full aspect-[16/9] object-cover rounded" alt="Jumping Quadruped">
                            </div>
                            <div class="flex-1 space-y-1">
                                <h3 class="text-gray-800">Autonomous Navigation with Optimized Jumping through Constrained Obstacles on Quadrupeds</h3>
                                <p class="text-gray-700 italic">IEEE Conference on Automation Science and Engineering (CASE) 2021.</p>
                                <p class="text-gray-600">Scott Gilroy, Derek Lau, Lizhi Yang, Ed Izaguirre, Kristen Biermayer, <strong>Anxing Xiao</strong>, Mengti Sun, Ayush Agrawal, Jun Zeng, Zhongyu Li, Koushil Sreenath</p>
                                <div class="pt-1">
                                    <p class="text-gray-700"><strong>TL;DR:</strong> Enable quadrupeds to jump through window-shaped obstacles during navigation.</p>
                                </div>
                                <div class="flex flex-wrap gap-2 pt-1">
                                    <a href="https://arxiv.org/abs/2107.00773" class="button" target="_blank">Paper</a>
                                    <a href="https://www.youtube.com/watch?v=5pzJ8U7YyGc" class="button" target="_blank">Video</a>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
    </main>
    <script>
        // Navbar active link highlight
        (function() {
            const path = (window.location.pathname.split('/').pop() || 'index.html');
            const links = document.querySelectorAll('header nav a');
            links.forEach(link => {
                const href = link.getAttribute('href');
                const isIndex = (path === '' || path === 'index.html');
                const matches = (isIndex && href.endsWith('index.html')) || href.endsWith(path);
                if (matches) {
                    link.classList.remove('text-blue-300');
                    link.classList.add('text-white','underline');
                }
            });
        })();
    </script>
    <script>
        // Research diagram hover interactions (removed line thickness change)
        document.addEventListener('DOMContentLoaded', function() {
            // Hover effects are now handled by CSS only
        });
    </script>
</body>
</html>


